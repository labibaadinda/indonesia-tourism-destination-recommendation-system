# -*- coding: utf-8 -*-
"""recommendation_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vohztyy01dBAPgNdGdwwsbQBPPO7ZiOs

# Surabaya Tourism Destination Recommendation System

- Nama  : Labiba Adinda Zahwana
- Email : labibaadinda

# Deskripsi Proyek

Proyek ini bertujuan membangun sistem rekomendasi destinasi wisata di Kota Surabaya yang dapat membantu pengguna dalam menemukan tempat wisata sesuai preferensi dan kebiasaan mereka.
Dataset yang digunakan pada proyek ini diperoleh dari public repository Kaggle yang dapat diakses melalui tautan berikut: [Indonesian Tourism Destination](https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination/code?datasetId=1481754&sortBy=voteCount).
<br>
Sistem ini menggunakan dua pendekatan utama:

1. **Content-Based Filtering**
   > Sistem memanfaatkan deskripsi dan kategori wisata untuk memberikan rekomendasi tempat wisata yang mirip secara konten dengan tempat yang sudah disukai pengguna. Proses ini melibatkan praproses teks menggunakan stemming dan penghilangan kata umum (stopword) Bahasa Indonesia, serta representasi fitur menggunakan TF-IDF dan perhitungan similarity cosine antar objek wisata.

2. **Collaborative Filtering dengan Deep Learning**
   > Menggunakan data rating dari pengguna terhadap destinasi wisata, dibuat model neural network dengan embedding untuk user dan tempat wisata guna mempelajari pola interaksi pengguna. Model ini memprediksi rating untuk tempat yang belum dikunjungi pengguna dan merekomendasikan tempat dengan prediksi rating tertinggi secara personal.

## 1. Import Library
Import library atau package yang akan digunakan pada project ini
"""

!pip install Sastrawi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import matplotlib.cm as cm
import warnings
warnings.filterwarnings('ignore')

import sklearn.feature_extraction.text as text
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import linear_kernel

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""Penjelasan library yang digunakan :
- pandas, numpy: Manipulasi data.

- matplotlib, seaborn, plotly.express: Visualisasi data.

- sklearn (TF-IDF, cosine similarity): Untuk content-based filtering.

- Sastrawi: Bahasa Indonesia stemmer dan stopword remover (praproses teks).

- tensorflow.keras: Membuat model deep learning untuk collaborative filtering.

## 2. Data Loading
"""

urls = [
    'https://raw.githubusercontent.com/labibaadinda/surabaya-tourism-destination-recommendation-system/main/dataset/package_tourism.csv',
    'https://raw.githubusercontent.com/labibaadinda/surabaya-tourism-destination-recommendation-system/main/dataset/tourism_rating.csv',
    'https://raw.githubusercontent.com/labibaadinda/surabaya-tourism-destination-recommendation-system/main/dataset/tourism_with_id.csv',
    'https://raw.githubusercontent.com/labibaadinda/surabaya-tourism-destination-recommendation-system/main/dataset/user.csv'
]

import pandas as pd

dataframes = [pd.read_csv(url) for url in urls]

data_tourism_rating = pd.read_csv(urls[1])
data_tourism_with_id = pd.read_csv(urls[2])
data_user = pd.read_csv(urls[3])

"""Data diambil dari URL (CSV online di Github Repository saya) ke dalam DataFrame pandas:

- package_tourism.csv: Paket wisata (tidak digunakan langsung di sini).

- tourism_rating.csv: Data rating pengguna ke tempat wisata.

- tourism_with_id.csv: Data tempat wisata dengan ID.

- user.csv: Data pengguna.

## 3. Data Understanding

- Menampilkan sekilas data (head()).

- Memeriksa ukuran data, tipe data, nilai null.

- Mengecek data duplikat dan menghapusnya jika ada (penting agar data bersih).
"""

data_tourism_rating.head()

data_tourism_with_id.head()

data_user.head()

print(f"Jumlah rating tourism: {data_tourism_rating.shape[0]}")
print(f"Jumlah tourism with id: {data_tourism_with_id.shape[0]}")
print(f"Jumlah users: {data_user.shape[0]}")

data_tourism_rating.info()

data_tourism_with_id.info()

data_user.info()

"""cek null di masing-masing dataframe"""

data_tourism_rating.isnull().sum().sort_values(ascending=False)

data_tourism_with_id.isnull().sum().sort_values(ascending=False)

data_user.isnull().sum().sort_values(ascending=False)

print(data_tourism_with_id.columns)

data_tourism_with_id['Category'].unique()

data_tourism_with_id.duplicated().sum()

data_tourism_rating.duplicated().sum()

data_user.duplicated().sum()

data_tourism_rating.duplicated().sum()

"""berdasarkan data understanding terdapat
 :
 - terdapat duplicated di data_tourism_rating, ini dihapus duplicated nya di bagian keempT data preprocessing (tepatnya di 4. 2. Data tourism rating)
 - terdapat null di data frame data_tourism_with_id dengan kolom (nantinya kolom tsb akan di hapus karena tidak digunakan untuk proses lebih lanjutnya)
* Unnamed: 11	sebanyak 437
* Time_Minutes sebanyak 232

## 4. Data Preprocessing

### 4. 1. Data tourism with id
"""

# drop kolom yang tidak terpakai
data_tourism_with_id.drop(['Rating','Time_Minutes','Coordinate','Lat','Long','Unnamed: 11','Unnamed: 12'],axis=1,inplace=True)
data_tourism_with_id

"""Ubah data agar hanya **Kota Surabaya** saja"""

place = data_tourism_with_id[data_tourism_with_id['City'] == 'Surabaya']

place.head()

"""data preprocessing pada 4.1 data tourism with id dilakukan **filtering** :
* Drop kolom yang tidak relevan untuk analisis.

* Fokus hanya pada wisata di Kota Surabaya dengan filter City == 'Surabaya'.

### 4. 2. Data tourism rating

**menghapus** data tourism rating yang duplikat
"""

data_tourism_rating.drop_duplicates(inplace=True)

data_tourism_rating.duplicated().sum()

# Merubah data rating agar hanya berisi rating pada tempat wisata dari Surabaya
rating = pd.merge(data_tourism_rating, place[['Place_Id']], on='Place_Id', how='right')
rating.head()

# Melihat ukuran dataset rating untuk Kota Surabaya
rating.shape

# gabungin antara rating dengan place (yang sudah difilter surabaya aja)
data_rekomendasi = pd.merge(rating.groupby('Place_Id')['Place_Ratings'].mean(),place,on='Place_Id')
data_rekomendasi

"""**Pada 4.2. Data tourism rating dilakukan preprocessing dengan cara:**
* Menghapus data tourism rating yang duplikat
* **Memfilter data rating** agar hanya berisi rating dari tempat-tempat wisata yang ada di Surabaya, dengan cara melakukan *merge* antara `data_tourism_rating` dan data `place` (yang sudah difilter hanya tempat wisata di Surabaya) berdasarkan `Place_Id`. Metode penggabungan menggunakan `how='right'` agar hasilnya hanya mencakup tempat wisata Surabaya.

* **Menghitung rata-rata rating** untuk tiap tempat wisata di Surabaya dengan mengelompokkan data rating berdasarkan `Place_Id` dan mengambil nilai rata-rata pada kolom `Place_Ratings`.

* **Menggabungkan hasil rata-rata rating** tersebut kembali dengan data `place` agar memperoleh data lengkap setiap tempat wisata beserta rata-rata ratingnya.

### 4. 3. Data User
"""

data_user.head()

# Merubah data user agar hanya berisi user yang pernah mengunjungi wisata di Surabaya
user = pd.merge(data_user, rating[['User_Id']], on='User_Id', how='right')
user.head()

# Melihat dataset user yang pernah memberi rating pada wisata di Surabaya

user.shape

"""Pada tahap preprocessing di data user dilakukan penyaringan data :
> Memfilter data user agar hanya mencakup pengguna yang pernah memberi rating destinasi wisata di Surabaya dengan menggabungkan data user dan data rating Surabaya berdasarkan User_Id.

## 5. Exploratory Data Analysis (EDA)
"""

# buat variable / data frame yang isi nya lokasi dengan jumlah rating terbanyak
top_10 = rating['Place_Id'].value_counts().sort_values(ascending=False).reset_index()[0:10]
top_10 = pd.merge(top_10, data_tourism_with_id[['Place_Id', 'Place_Name']], on='Place_Id', how='left')
top_10

# Membuat visualisasi wisata dengan jumlah rating terbanyak
plt.figure(figsize=(10,6))

colors = sns.color_palette([
    "#ffb6c1",  # light pink
    "#f8bbd0",  # pastel pink
    "#f3c6d1",  # soft rose
    "#eac7d8",  # dusty pink
    "#fce4ec",  # very light pink
    "#a8dadc",  # pastel blue mint (pelengkap)
    "#ffe5d9",  # pastel peach (pelengkap)
    "#f6d1c1",  # pastel coral (pelengkap)
    "#c1b7a1",  # beige soft (pelengkap)
    "#d9ead3"   # pastel green (pelengkap)
])


sns.barplot(
    x='Place_Id',
    y='Place_Name',
    data=top_10,
    palette=colors
)

plt.title('Jumlah Tempat Wisata dengan Rating Terbanyak', pad=20)
plt.ylabel('Nama Lokasi')
plt.xlabel('Jumlah Rating')
plt.tight_layout()
plt.show()

# Membuat visualisasi jumlah kategori wisata di Kota Surabaya
colors = sns.color_palette([
    "#ffb6c1",  # light pink
    "#f8bbd0",  # pastel pink
    "#f3c6d1",  # soft rose
    "#eac7d8",  # dusty pink
    "#fce4ec",  # very light pink
    "#a8dadc",  # pastel blue mint (pelengkap)
    "#ffe5d9",  # pastel peach (pelengkap)
    "#f6d1c1",  # pastel coral (pelengkap)
    "#c1b7a1",  # beige soft (pelengkap)
    "#d9ead3"   # pastel green (pelengkap)
])

sns.countplot(
    y='Category',
    data=place,
    palette= colors
)

plt.title('Perbandingan Jumlah Kategori Wisata di Kota Surabaya', pad=20)
plt.show()

plt.figure(figsize=(5,3))
sns.boxplot(x='Age', data=user, color='pink')
plt.title('Distribusi Usia User', pad=20)
plt.show()

# Membuat visualisasi distribusi harga masuk tempat wisata

plt.figure(figsize=(7,3))
sns.boxplot(x=place['Price'], color='pink')
plt.title('Distribusi Harga Masuk Wisata di Kota Surabaya', pad=20)
plt.show()

# Memfilter asal kota dari user
# Buat palette warna pinkish pastel (custom)
colors = sns.color_palette([
    "#ffb6c1",  # light pink
    "#f8bbd0",  # pastel pink
    "#f3c6d1",  # soft rose
    "#eac7d8",  # dusty pink
    "#fce4ec",  # very light pink
    "#a8dadc",  # pastel blue mint (pelengkap)
    "#ffe5d9",  # pastel peach (pelengkap)
    "#f6d1c1",  # pastel coral (pelengkap)
    "#c1b7a1",  # beige soft (pelengkap)
    "#d9ead3"   # pastel green (pelengkap)
])


askot = user['Location'].apply(lambda x : x.split(',')[0])

# Visualisasi asal kota dari user
plt.figure(figsize=(8,6))
sns.countplot(
    y=askot,
    data=user,
    palette=colors
)
plt.title('Jumlah Asal Kota dari User')
plt.show()

"""Pada EDA ini telah dilakukan :
- Visualisasi top 10 tempat wisata berdasarkan jumlah rating.

- Visualisasi kategori wisata yang ada di Surabaya.

- Distribusi usia pengguna.

- Distribusi harga tiket masuk.

- Visualisasi asal kota pengguna.

Hasil **analisis** EDA :
- tempat paling banyak pengunjung di Surabaya `Gereja Perawan Maria Tak Berdosa Surabaya`.
- jumlah kategori wisata di Kota Surabaya terbanyak yaitu `budaya` dan `taman hiburan`.
- distribusi usia sekitar 23-34 tahun.
- distribusi Harga Masuk Wisata di Kota Surabaya sekitar 0-20000, namun ada beberapa outlier / harga diluar range tsb.
- asal kota dari User terbanyak ada di Kota `Bekasi`.

## 6. Data Preparation

**Buat fungsi preprocessing data**
"""

stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

def preprocessing(data):
  data = data.lower()
  data = stem.stem(data)
  data = stopword.remove(data)
  return data

data_content_based_filtering = data_rekomendasi.copy()
data_content_based_filtering['Tags'] = data_content_based_filtering['Description'] + " " + data_content_based_filtering['Category']
data_content_based_filtering.drop(['Price','Place_Ratings','Description','Category','City'],axis=1,inplace=True)
data_content_based_filtering

"""**Melakukan Encoding dengan TF-IDF**"""

# buat variable tf untuk mempermudah pemanggilan method TfidfVectorizer
tv = TfidfVectorizer(max_features=5000)

vectors = tv.fit_transform(data_content_based_filtering.Tags).toarray()
vectors

"""**Ekstraksi Fitur Teks**
> Menggunakan TF-IDF Vectorizer (Term Frequency - Inverse Document Frequency) yang mengubah teks menjadi representasi numerik berbentuk vektor.
- TF-IDF membantu menimbang kata-kata penting yang membedakan satu tempat wisata dengan yang lain.
- Batasi fitur maksimal (max_features=5000) agar model efisien dan tidak terlalu rumit.

## 7. Model Development

### 7. 1. Content Based Filtering

Content-Based Filtering merekomendasikan item (tempat wisata) berdasarkan kemiripan konten (fitur deskriptif) dari item yang sudah disukai atau pernah dikunjungi oleh pengguna.
"""

similarity = cosine_similarity(vectors)
similarity[0][1:10]

"""**Menghitung Similarity**
> Dengan vektor TF-IDF dari semua tempat wisata, dihitung cosine similarity antar tempat wisata.
- Cosine similarity mengukur seberapa mirip dua destinasi berdasarkan konten teksnya, dengan nilai antara 0 (tidak mirip) sampai 1 (sangat mirip).
"""

def recommend_by_content_based_filtering(nama_tempat):
    nama_tempat_index = data_content_based_filtering[data_content_based_filtering['Place_Name']==nama_tempat].index[0]
    distancess = similarity[nama_tempat_index]
    nama_tempat_list = sorted(list(enumerate(distancess)),key=lambda x: x[1],reverse=True)[1:10]

    recommended_nama_tempats = []
    for i in nama_tempat_list:
        recommended_nama_tempats.append([data_content_based_filtering.iloc[i[0]].Place_Name]+[i[1]])
        # print(nama_tempats.iloc[i[0]].original_title)

    return recommended_nama_tempats

data_content_based_filtering['Place_Name'].unique()

# tes fungsi content based filtering
recommend_by_content_based_filtering('Taman Bungkul')

"""telah **berhasil** membuat rekomendasi berbasis content filtering

**Rekomendasi  content based filtering**
> Saat user memilih atau menyukai suatu destinasi, sistem akan merekomendasikan destinasi lain yang memiliki nilai similarity tinggi dengan destinasi tersebut.
- Misalnya, jika user suka “Taman Bungkul,” sistem akan merekomendasikan tempat-tempat yang deskripsi dan kategorinya mirip dengan Taman Bungkul.

### 7. 2.  Collaborative Filtering

*Collaborative Filtering* (CF) merekomendasikan item (tempat wisata) kepada pengguna berdasarkan pola interaksi atau preferensi pengguna lain yang serupa. Intinya, “pengguna dengan selera mirip biasanya menyukai tempat yang sama.”
"""

data_collaborative_filtering = rating.copy()
data_collaborative_filtering

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = data_collaborative_filtering['User_Id'].unique().tolist()
print(f'Jumlah user: {len(user_ids)} (ID dari 1 hingga {user_ids[-1]})')

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print(f'user_to_user_encoded: {user_to_user_encoded}')

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print(f'user_encoded_to_user: {user_encoded_to_user}')

# Mengubah placeID menjadi list tanpa nilai yang sama
resto_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
resto_to_resto_encoded = {x: i for i, x in enumerate(resto_ids)}

# Melakukan proses encoding angka ke placeID
resto_encoded_to_resto = {i: x for i, x in enumerate(resto_ids)}

# Mapping userID ke dataframe user
data_collaborative_filtering['user'] = data_collaborative_filtering['User_Id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe resto
data_collaborative_filtering['resto'] = data_collaborative_filtering['Place_Id'].map(resto_to_resto_encoded)

"""**Data Interaksi**
> Menggunakan data rating pengguna terhadap destinasi wisata (User_Id, Place_Id, Place_Ratings).
- Rating ini menunjukkan seberapa suka user terhadap suatu tempat.

**Encoding User dan Item**
> User dan tempat wisata diubah dari ID asli menjadi indeks numerik (misal User_Id 101 → indeks 0, dst).
- Ini penting agar bisa dipakai sebagai input embedding pada model neural network.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_resto = len(resto_encoded_to_resto)
print(num_resto)

# Mengubah rating menjadi nilai float
data_collaborative_filtering['Place_Ratings'] = data_collaborative_filtering['Place_Ratings'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data_collaborative_filtering['Place_Ratings'])

# Nilai maksimal rating
max_rating = max(data_collaborative_filtering['Place_Ratings'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_resto, min_rating, max_rating
))

"""**Normalisasi Rating**
> Rating diubah ke skala 0-1 agar proses training model lebih stabil dan sesuai dengan fungsi aktivasi sigmoid.
"""

data_collaborative_filtering = data_collaborative_filtering.sample(frac=1, random_state=42)
data_collaborative_filtering

x = data_collaborative_filtering[['user', 'resto']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data_collaborative_filtering['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * data_collaborative_filtering.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""**Modeling**"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_resto, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""**Model Neural Network dengan Embedding**
- Dibangun model RecommenderNet dengan dua embedding layer: satu untuk user dan satu untuk tempat wisata.
- Embedding ini memetakan user dan tempat ke dalam ruang vektor berdimensi rendah yang merepresentasikan karakteristik dan preferensi.
- Model mempelajari interaksi antara user dan tempat melalui operasi dot product dari embedding yang dipelajari.

**Training Model**
- Model dilatih memprediksi rating pengguna pada tempat wisata tertentu berdasarkan embedding.
- Fungsi loss yang dipakai adalah binary crossentropy, dengan metrik evaluasi Root Mean Squared Error (RMSE).
"""

import matplotlib.pyplot as plt
import numpy as np

# Ambil data RMSE dari history
train_rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']
epochs = range(1, len(train_rmse) + 1)

plt.figure(figsize=(12, 6))

# Plot train RMSE
plt.plot(epochs, train_rmse, 'b-', label='Train RMSE', linewidth=2)

# Plot validation RMSE
plt.plot(epochs, val_rmse, 'r--', label='Validation RMSE', linewidth=2)

# Titik terbaik validation RMSE (epoch dengan nilai val RMSE terkecil)
best_epoch = np.argmin(val_rmse) + 1
best_val_rmse = val_rmse[best_epoch - 1]

plt.scatter(best_epoch, best_val_rmse, color='green', s=100, zorder=5,
            label=f'Best Val RMSE\nEpoch {best_epoch}: {best_val_rmse:.4f}')

plt.title('Training and Validation RMSE over Epochs', fontsize=16)
plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Root Mean Squared Error (RMSE)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""Penjelasan grafik plot :
- Garis biru solid = RMSE training

- Garis merah putus-putus = RMSE validasi

- Titik hijau menandai epoch terbaik berdasarkan validasi RMSE (nilai terendah)

- Plot menunjukkan bahwa nilai RMSE training dan validasi cukup dekat dan stabil, menandakan model tidak overfitting atau underfitting parah.
- `Best Epoch` berdasarkan Val RMSE adalah Epoch ke 11
"""

resto_df = data_tourism_with_id
df = data_tourism_rating

# Mengambil sample user
user_id = df.User_Id.sample(1).iloc[0]
resto_visited_by_user = df[df.User_Id == user_id]

resto_not_visited = resto_df[~resto_df['Place_Id'].isin(resto_visited_by_user.Place_Id.values)]['Place_Id']
resto_not_visited = list(
    set(resto_not_visited)
    .intersection(set(resto_to_resto_encoded.keys()))
)

resto_not_visited = [[resto_to_resto_encoded.get(x)] for x in resto_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_resto_array = np.hstack(
    ([[user_encoder]] * len(resto_not_visited), resto_not_visited)
)

ratings = model.predict(user_resto_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_resto_ids = [
    resto_encoded_to_resto.get(resto_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Resto with high ratings from user')
print('----' * 8)

top_resto_user = (
    resto_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(10)
    .Place_Id.values
)

resto_df_rows = resto_df[resto_df['Place_Id'].isin(top_resto_user)]
for row in resto_df_rows.itertuples():
    print(row.Place_Name)

print('----' * 8)
print('Top 10 resto recommendation')
print('----' * 8)

recommended_resto = resto_df[resto_df['Place_Id'].isin(recommended_resto_ids)]
for row in recommended_resto.itertuples():
    print(row.Place_Name)

def get_bulk_recommendations(model, user_ids, user_encoder, item_encoder, item_decoder, user_item_df, item_df, top_k=10):
    """
    Fungsi rekomendasi top_k item untuk banyak user sekaligus.

    Args:
    - model: trained keras model (RecommenderNet)
    - user_ids: list/array user_id asli (misal semua user dalam dataset)
    - user_encoder: dict mapping user_id ke encoded index
    - item_encoder: dict mapping item_id ke encoded index
    - item_decoder: dict mapping encoded index ke item_id
    - user_item_df: DataFrame rating dengan kolom ['User_Id','Place_Id','Place_Ratings']
    - item_df: DataFrame item lengkap (nama, id, dll)
    - top_k: jumlah rekomendasi per user

    Returns:
    - dict: {user_id: [list rekomendasi nama item]}
    """

    all_recommendations = {}

    for user_id in user_ids:
        if user_id not in user_encoder:
            all_recommendations[user_id] = []
            continue

        visited = user_item_df[user_item_df['User_Id'] == user_id]['Place_Id'].values
        not_visited = item_df[~item_df['Place_Id'].isin(visited)]['Place_Id']
        not_visited = list(set(not_visited).intersection(set(item_encoder.keys())))

        if len(not_visited) == 0:
            all_recommendations[user_id] = []
            continue

        user_encoded = user_encoder[user_id]
        not_visited_encoded = [[item_encoder[x]] for x in not_visited]

        user_input = np.hstack((np.array([[user_encoded]] * len(not_visited_encoded)), not_visited_encoded))

        preds = model.predict(user_input).flatten()
        top_indices = preds.argsort()[-top_k:][::-1]

        recommended_ids = [not_visited[i] for i in top_indices]
        recommended_names = item_df[item_df['Place_Id'].isin(recommended_ids)]['Place_Name'].values

        all_recommendations[user_id] = recommended_names.tolist()

    return all_recommendations

all_users = data_collaborative_filtering['User_Id'].unique()

bulk_rekom = get_bulk_recommendations(
    model=model,
    user_ids=all_users,
    user_encoder=user_to_user_encoded,
    item_encoder=resto_to_resto_encoded,
    item_decoder=resto_encoded_to_resto,
    user_item_df=data_collaborative_filtering,
    item_df=data_tourism_with_id,
    top_k=10
)

# Contoh print rekomendasi user pertama
first_user = all_users[0]
print('----------------------------------------')
print(f"Rekomendasi untuk user {first_user}:")
print('----------------------------------------')
print(bulk_rekom[first_user])

def get_recommendations_for_user(model, user_id, user_encoder, item_encoder, item_decoder, user_item_df, item_df, top_k=10):
    """
    Fungsi untuk rekomendasi top_k item (tempat wisata) untuk user tertentu.

    Args:
    - model: trained keras model (RecommenderNet)
    - user_id: id asli user (misal 'User_Id')
    - user_encoder: dict mapping user_id ke encoded index
    - item_encoder: dict mapping item_id (Place_Id) ke encoded index
    - item_decoder: dict mapping encoded index ke item_id (Place_Id)
    - user_item_df: DataFrame rating dengan kolom ['User_Id','Place_Id','Place_Ratings']
    - item_df: DataFrame item (tempat wisata) lengkap, untuk ambil nama dsb
    - top_k: jumlah rekomendasi yang diinginkan (default=10)

    Returns:
    - List nama item yang direkomendasikan
    """

    if user_id not in user_encoder:
        return f"User ID {user_id} tidak ditemukan dalam data."

    # Tempat yang sudah pernah dikunjungi user
    visited = user_item_df[user_item_df['User_Id'] == user_id]['Place_Id'].values

    # Tempat yang belum dikunjungi
    not_visited = item_df[~item_df['Place_Id'].isin(visited)]['Place_Id']
    not_visited = list(set(not_visited).intersection(set(item_encoder.keys())))

    # Encode user dan tempat yang belum dikunjungi
    user_encoded = user_encoder[user_id]
    not_visited_encoded = [[item_encoder[x]] for x in not_visited]

    # Bentuk array input untuk model
    user_input = np.hstack((np.array([[user_encoded]] * len(not_visited_encoded)), not_visited_encoded))

    # Prediksi rating untuk tempat belum dikunjungi
    preds = model.predict(user_input).flatten()

    # Urutkan indeks rating tertinggi
    top_indices = preds.argsort()[-top_k:][::-1]

    # Decode item_id rekomendasi
    recommended_ids = [not_visited[i] for i in top_indices]

    # Ambil nama tempat dari item_df
    recommended_names = item_df[item_df['Place_Id'].isin(recommended_ids)]['Place_Name'].values

    return recommended_names.tolist()

recommendations = get_recommendations_for_user(
    model=model,
    user_id=123,
    user_encoder=user_to_user_encoded,
    item_encoder=resto_to_resto_encoded,
    item_decoder=resto_encoded_to_resto,
    user_item_df=data_collaborative_filtering,
    item_df=data_tourism_with_id,
    top_k=10
)
print('-----------------------------------------------------')
print("Tempat Top 10 Rekomendasi untuk user dengan ID 123:")
print('-----------------------------------------------------')
for place in recommendations:
    print(place)

"""**Berhasil Membuat Rekomendasi**
- Untuk user tertentu, model memprediksi rating untuk semua tempat yang belum dikunjungi.
- Tempat dengan prediksi rating tertinggi direkomendasikan ke user tersebut.

**Keunggulan Collaborative Filtering**
- Menangkap preferensi tersembunyi dari pola rating banyak pengguna tanpa perlu data atribut item (deskripsi, kategori).
- Dapat memberikan rekomendasi personal yang adaptif mengikuti perilaku dan selera pengguna.
- Bisa merekomendasikan item yang berbeda jauh dari yang sudah pernah user lihat (eksplorasi).

**Keterbatasan**
- Membutuhkan data interaksi (rating) yang cukup banyak agar model efektif.
- Cold start problem untuk user atau item baru yang belum punya data rating.
- Model perlu pelatihan ulang jika data rating baru masuk.

## 7. Conclusion

Sistem rekomendasi destinasi wisata Kota Surabaya **berhasil** dikembangkan menggunakan metode **Content-Based Filtering** dan **Collaborative Filtering** berbasis deep learning.

* Melalui eksplorasi data dan preprocessing yang tepat, model menunjukkan performa yang stabil dan akurat berdasarkan metrik Root Mean Squared Error (RMSE) pada data training dan validasi.

* Visualisasi data dan hasil rekomendasi memperkuat pemahaman tentang karakteristik pengguna dan destinasi wisata, serta memberikan insight yang berguna untuk pengembangan pariwisata di Surabaya.

* **Content-Based Filtering** mampu memberikan rekomendasi berdasarkan kemiripan deskripsi dan kategori tempat wisata, sehingga membantu pengguna menemukan tempat baru yang serupa dengan preferensi mereka.

* **Collaborative Filtering** dengan model neural network embedding berhasil menangkap pola interaksi antara pengguna dan destinasi wisata melalui data rating, menghasilkan rekomendasi yang personal dan adaptif.
"""